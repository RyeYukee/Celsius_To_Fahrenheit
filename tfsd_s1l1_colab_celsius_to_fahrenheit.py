# -*- coding: utf-8 -*-
"""TFSD_S1L1_Colab_Celsius_To_Fahrenheit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJM69dqDxiLGtWQY5snv4N1cTxbm-w13
"""

import tensorflow as tf
tf.logging.set_verbosity(tf.logging.ERROR) # 仅记录错误信息
import numpy as np

celsius_q    = np.array([-40, -10,  0,  8, 15, 22,  38],dtype=float)
fahrenheit_a = np.array([-40,  14, 32, 46, 59, 72, 100],dtype=float)
for i,c in enumerate(celsius_q):
  print("{} degrees Celsius = {} degrees Fahrenhet".format(c,fahrenheit_a[i]))

"""### Some Machine Learning teriminology(一些机器学习专用术语)

- 特征：模型的输入
- 样本：用于训练流程的输入/输出对
- 标签：模型的输出
- 层级：神经网络中相互连接的节点集合。
- 模型：神经网络的表示法
- 密集全连接层 (FC)：一个层级中的每个节点都与上个层级中的每个节点相连。
- 权重和偏差：模型的内部变量
- 损失：期望输出和真实输出之间的差值
- MSE：均方误差，一种损失函数，它会将一小部分很大的差值视作比大量很小的差值更糟糕。
- 梯度下降法：每次小幅调整内部变量，从而逐渐降低损失函数的算法。
- 优化器：梯度下降法的一种具体实现方法。（有很多算法。在这门课程中，我们将仅使用“Adam”优化器，它是 ADAptive with Momentum 的简称，并且被视为最佳优化器。）
- 学习速率：梯度下降过程中的损失改进“步长”。
- 批次：在训练神经网络的过程中使用的一组样本。
- 周期：完全经过整个训练数据集一轮
- 前向传播：根据输入计算输出值
- 反向传播：根据优化器算法计算内部变量的调整幅度，从输出层级开始，并往回计算每个层级，直到抵达输入层。
"""

l0 = tf.keras.layers.Dense(units=1, input_shape=[1]) # units指定了这个层级将有多少内部变量

model = tf.keras.Sequential([l0])

model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(0.1))

history = model.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)#有7个映射样本，所以模型一共训练3500个样本
print("Finished training the model")

import matplotlib.pyplot as plt
plt.xlabel('Epoch Number')
plt.ylabel('Loss Magnitude')
plt.plot(history.history['loss'])

print(model.predict([100.0]))

print("These are the layer variables: {}".format(l0.get_weights()))